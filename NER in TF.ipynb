{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMW198dQxMVe2Ncmp5w9ese"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6iONlhP1gP2U","executionInfo":{"status":"ok","timestamp":1692410128837,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"outputs":[],"source":["import pickle"]},{"cell_type":"code","source":["# conll 2003\n","!wget -nc https://lazyprogrammer.me/course_files/nlp/ner_train.pkl\n","!wget -nc https://lazyprogrammer.me/course_files/nlp/ner_test.pkl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7F1AGYGugQyo","executionInfo":{"status":"ok","timestamp":1692410135118,"user_tz":-300,"elapsed":2286,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"05d6976b-2e15-4b39-d767-cf4bb7b6882c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-08-19 01:55:32--  https://lazyprogrammer.me/course_files/nlp/ner_train.pkl\n","Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166, 2606:4700:3030::ac43:d5a6, ...\n","Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4757208 (4.5M) [application/octet-stream]\n","Saving to: ‘ner_train.pkl’\n","\n","ner_train.pkl       100%[===================>]   4.54M  5.14MB/s    in 0.9s    \n","\n","2023-08-19 01:55:33 (5.14 MB/s) - ‘ner_train.pkl’ saved [4757208/4757208]\n","\n","--2023-08-19 01:55:33--  https://lazyprogrammer.me/course_files/nlp/ner_test.pkl\n","Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166, 2606:4700:3030::ac43:d5a6, ...\n","Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1201978 (1.1M) [application/octet-stream]\n","Saving to: ‘ner_test.pkl’\n","\n","ner_test.pkl        100%[===================>]   1.15M  2.33MB/s    in 0.5s    \n","\n","2023-08-19 01:55:34 (2.33 MB/s) - ‘ner_test.pkl’ saved [1201978/1201978]\n","\n"]}]},{"cell_type":"code","source":["with open('ner_train.pkl', 'rb') as f:\n","  corpus_train = pickle.load(f)\n","\n","with open('ner_test.pkl', 'rb') as f:\n","  corpus_test = pickle.load(f)"],"metadata":{"id":"zMiTA4YZgTO7","executionInfo":{"status":"ok","timestamp":1692410137926,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["train_inputs = []\n","train_targets = []\n","\n","for sentence_tag_pairs in corpus_train:\n","  tokens = []\n","  target = []\n","  for token, tag in sentence_tag_pairs:\n","    tokens.append(token)\n","    target.append(tag)\n","  train_inputs.append(tokens)\n","  train_targets.append(target)"],"metadata":{"id":"zSwC-o62gUc1","executionInfo":{"status":"ok","timestamp":1692410144803,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["test_inputs = []\n","test_targets = []\n","\n","for sentence_tag_pairs in corpus_test:\n","  tokens = []\n","  target = []\n","  for token, tag in sentence_tag_pairs:\n","    tokens.append(token)\n","    target.append(tag)\n","  test_inputs.append(tokens)\n","  test_targets.append(target)"],"metadata":{"id":"V5ZuZkVGgWRF","executionInfo":{"status":"ok","timestamp":1692410148160,"user_tz":-300,"elapsed":1,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Input, Bidirectional\n","from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Embedding\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy"],"metadata":{"id":"-kpmyVWcgXF7","executionInfo":{"status":"ok","timestamp":1692410158153,"user_tz":-300,"elapsed":6520,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Convert sentences to sequences\n","\n","MAX_VOCAB_SIZE = None\n","\n","# capitalization might be useful - test it\n","should_lowercase = False\n","word_tokenizer = Tokenizer(\n","    num_words=MAX_VOCAB_SIZE,\n","    lower=should_lowercase,\n","    oov_token='UNK',\n",")\n","# otherwise unknown tokens will be removed and len(input) != len(target)\n","# input words and target words will not be aligned!\n","\n","# it's ok to \"fit\" on the whole corpus - it just means some embeddings\n","# won't be trained\n","# this is because for the test set, any unknown tokens will be removed\n","# which will change the length of the input (***CHECK)\n","\n","word_tokenizer.fit_on_texts(train_inputs)\n","train_inputs_int = word_tokenizer.texts_to_sequences(train_inputs)\n","test_inputs_int = word_tokenizer.texts_to_sequences(test_inputs)"],"metadata":{"id":"Zp_IcollgX5h","executionInfo":{"status":"ok","timestamp":1692410158544,"user_tz":-300,"elapsed":392,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# get word -> integer mapping\n","word2idx = word_tokenizer.word_index\n","V = len(word2idx)\n","print('Found %s unique tokens.' % V)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtX9I-AGgZCF","executionInfo":{"status":"ok","timestamp":1692410159411,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"3d33acf7-030c-4d15-a97c-cf02e15b83f3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 23299 unique tokens.\n"]}]},{"cell_type":"code","source":["# https://stackoverflow.com/questions/11264684/flatten-list-of-lists\n","def flatten(list_of_lists):\n","  flattened = [val for sublist in list_of_lists for val in sublist]\n","  return flattened"],"metadata":{"id":"qXDojp0TgZ1p","executionInfo":{"status":"ok","timestamp":1692410162483,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["all_train_targets = set(flatten(train_targets))\n","all_train_targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Szt2qcgDgajF","executionInfo":{"status":"ok","timestamp":1692410167195,"user_tz":-300,"elapsed":3,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"6a5049c3-67e9-4852-a45c-c471e552d7b5"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["all_test_targets = set(flatten(test_targets))\n","all_test_targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WA_RhdQ0gbtL","executionInfo":{"status":"ok","timestamp":1692410172057,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"38b8ab06-387b-4ce0-a1f3-6b1223eccc30"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["all_train_targets == all_test_targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUbMdI3qgc33","executionInfo":{"status":"ok","timestamp":1692410175075,"user_tz":-300,"elapsed":3,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"591b8307-4832-4768-fb4c-37cf033e55a4"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Convert targets to sequences\n","tag_tokenizer = Tokenizer()\n","tag_tokenizer.fit_on_texts(train_targets)\n","train_targets_int = tag_tokenizer.texts_to_sequences(train_targets)\n","test_targets_int = tag_tokenizer.texts_to_sequences(test_targets)\n","\n","# save for later\n","train_targets_int_unpadded = train_targets_int\n","test_targets_int_unpadded = test_targets_int"],"metadata":{"id":"IYZI8Rhfgdn3","executionInfo":{"status":"ok","timestamp":1692410178025,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# before padding, find max document length\n","# because we don't want to truncate any inputs\n","# which would also truncate targets\n","maxlen_train = max(len(sent) for sent in train_inputs)\n","maxlen_test = max(len(sent) for sent in test_inputs)\n","T = max((maxlen_train, maxlen_test))"],"metadata":{"id":"upekvtvGgeVn","executionInfo":{"status":"ok","timestamp":1692410180792,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# pad sequences so that we get a N x T matrix\n","train_inputs_int = pad_sequences(train_inputs_int, maxlen=T)\n","print('Shape of data train tensor:', train_inputs_int.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WzlnxqOgfHh","executionInfo":{"status":"ok","timestamp":1692410185036,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"8e282dc7-1c0f-44ea-9087-efb6f4a8beab"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data train tensor: (12733, 113)\n"]}]},{"cell_type":"code","source":["test_inputs_int = pad_sequences(test_inputs_int, maxlen=T)\n","print('Shape of data test tensor:', test_inputs_int.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67yX9HJBgf6P","executionInfo":{"status":"ok","timestamp":1692410187111,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"b0af64db-5e0e-4ea2-8554-a6d3674f311f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of data test tensor: (2970, 113)\n"]}]},{"cell_type":"code","source":["train_targets_int = pad_sequences(train_targets_int, maxlen=T)\n","print('Shape of train targets tensor:', train_targets_int.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3Cdky8vggjy","executionInfo":{"status":"ok","timestamp":1692410191943,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"a9489ab8-1fbf-4c94-dad1-00111d0a93a0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of train targets tensor: (12733, 113)\n"]}]},{"cell_type":"code","source":["test_targets_int = pad_sequences(test_targets_int, maxlen=T)\n","print('Shape of test targets tensor:', test_targets_int.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DPWT9I0ghvU","executionInfo":{"status":"ok","timestamp":1692410196419,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"489a1243-2daf-47a3-9feb-3c4f7a34f5e5"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of test targets tensor: (2970, 113)\n"]}]},{"cell_type":"code","source":["# number of classes\n","K = len(tag_tokenizer.word_index) + 1\n","K"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-kc7YDwgioG","executionInfo":{"status":"ok","timestamp":1692410198176,"user_tz":-300,"elapsed":2,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}},"outputId":"c4921784-b61b-41a5-d8d2-f4bce4b210c2"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Create the model\n","\n","# We get to choose embedding dimensionality\n","D = 32\n","\n","# Note: we actually want to the size of the embedding to (V + 1) x D,\n","# because the first index starts from 1 and not 0.\n","# Thus, if the final index of the embedding matrix is V,\n","# then it actually must have size V + 1.\n","\n","i = Input(shape=(T,))\n","# mask_zero=True way slower on GPU than CPU!\n","x = Embedding(V + 1, D, mask_zero=True)(i)\n","x = Bidirectional(LSTM(32, return_sequences=True))(x)\n","# x = SimpleRNN(32, return_sequences=True)(x)\n","x = Dense(K)(x)\n","\n","model = Model(i, x)"],"metadata":{"id":"9WkFY1s7gjSb","executionInfo":{"status":"ok","timestamp":1692410205256,"user_tz":-300,"elapsed":2215,"user":{"displayName":"hamzah tariq","userId":"00184607934785050640"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Compile and fit\n","model.compile(\n","  loss=SparseCategoricalCrossentropy(from_logits=True),\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")\n","\n","# 60s per epoch on CPU\n","\n","print('Training model...')\n","r = model.fit(\n","  train_inputs_int,\n","  train_targets_int,\n","  epochs=5,\n","  validation_data=(test_inputs_int, test_targets_int)\n",")"],"metadata":{"id":"edtCFl88gkcq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot loss per iteration\n","plt.plot(r.history['loss'], label='train loss')\n","plt.plot(r.history['val_loss'], label='val loss')\n","plt.legend();"],"metadata":{"id":"0_Wfq5l-gnIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot accuracy per iteration\n","plt.plot(r.history['accuracy'], label='train acc')\n","plt.plot(r.history['val_accuracy'], label='val acc')\n","plt.legend();"],"metadata":{"id":"0SdxHRKJgnPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# True model accuracy - above includes padding\n","\n","# first get length of each sequence\n","train_lengths = []\n","for sentence in train_inputs:\n","  train_lengths.append(len(sentence))\n","\n","test_lengths = []\n","for sentence in test_inputs:\n","  test_lengths.append(len(sentence))"],"metadata":{"id":"1GEmx_cygnVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_probs = model.predict(train_inputs_int) # N x T x K\n","train_predictions = []\n","for probs, length in zip(train_probs, train_lengths):\n","  # probs is T x K\n","  probs_ = probs[-length:]\n","  preds = np.argmax(probs_, axis=1)\n","  train_predictions.append(preds)\n","\n","# flatten\n","flat_train_predictions = flatten(train_predictions)\n","flat_train_targets = flatten(train_targets_int_unpadded)"],"metadata":{"id":"eoolfCB1gnbc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_probs = model.predict(test_inputs_int) # N x T x K\n","test_predictions = []\n","for probs, length in zip(test_probs, test_lengths):\n","  # probs is T x K\n","  probs_ = probs[-length:]\n","  preds = np.argmax(probs_, axis=1)\n","  test_predictions.append(preds)\n","\n","# flatten\n","flat_test_predictions = flatten(test_predictions)\n","flat_test_targets = flatten(test_targets_int_unpadded)"],"metadata":{"id":"pT7JJtQNgnfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score\n","\n","print(\"Train acc:\", accuracy_score(flat_train_targets, flat_train_predictions))\n","print(\"Test acc:\", accuracy_score(flat_test_targets, flat_test_predictions))\n","\n","print(\"Train f1:\", f1_score(flat_train_targets, flat_train_predictions, average='macro'))\n","print(\"Test f1:\", f1_score(flat_test_targets, flat_test_predictions, average='macro'))"],"metadata":{"id":"fgY_2-pWgnkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Baseline model: map word to tag\n","from collections import Counter\n","\n","# https://stackoverflow.com/questions/1518522/find-the-most-common-element-in-a-list\n","def most_common(lst):\n","    data = Counter(lst)\n","    return data.most_common(1)[0][0]"],"metadata":{"id":"7woddi5vhAu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token2tags = {k: [] for k, v in word2idx.items()}\n","\n","# remove UNK token\n","del token2tags['UNK']\n","\n","for tokens, tags in zip(train_inputs, train_targets):\n","  for token, tag in zip(tokens, tags):\n","    if should_lowercase:\n","      token = token.lower()\n","    if token in token2tags:\n","      token2tags[token].append(tag)\n","\n","for k, v in token2tags.items():\n","  if len(v) == 0:\n","    print(k)\n","\n","token2tag = {k: most_common(v) for k, v in token2tags.items()}\n"],"metadata":{"id":"pBjNKshYhAz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute accuracy\n","\n","train_predictions = []\n","for sentence in train_inputs:\n","  predictions = []\n","  for token in sentence:\n","    if should_lowercase:\n","      token = token.lower()\n","    predicted_tag = token2tag[token]\n","    predictions.append(predicted_tag)\n","  train_predictions.append(predictions)\n","flat_train_predictions = flatten(train_predictions)\n","flat_train_targets = flatten(train_targets)"],"metadata":{"id":"9ekveokAhA6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_predictions = []\n","for sentence in test_inputs:\n","  predictions = []\n","  for token in sentence:\n","    predicted_tag = token2tag.get(token, 'INCORRECT')\n","    predictions.append(predicted_tag)\n","  test_predictions.append(predictions)\n","flat_test_predictions = flatten(test_predictions)\n","flat_test_targets = flatten(test_targets)"],"metadata":{"id":"GfSiU1s7hFiH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Train acc:\", accuracy_score(flat_train_targets, flat_train_predictions))\n","print(\"Test acc:\", accuracy_score(flat_test_targets, flat_test_predictions))\n","\n","print(\"Train f1:\", f1_score(flat_train_targets, flat_train_predictions, average='macro'))\n","print(\"Test f1:\", f1_score(flat_test_targets, flat_test_predictions, average='macro'))"],"metadata":{"id":"4FyipvDxhFlL"},"execution_count":null,"outputs":[]}]}